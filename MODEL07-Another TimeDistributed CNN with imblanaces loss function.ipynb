{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "TRAIN_P='../input/train/'\nTRAIN_T='../input/test/'\ntrain_df=pd.read_csv('../input/train.csv')\nprint(train_df[0:5].Id)\n#train_df=train_df[0:5000]\nprint(train_df.shape)\n#we treat this like unet arquitect and like a mask we replace target to 0 to 27 outputs from 0 to 1 values\nNUM_CLASSES=28\nNUM_SLIDES=16\ny_df=[[int(i) for i in s.split()] for s in train_df['Target']]\ny_df_final=np.zeros((train_df.shape[0],NUM_CLASSES)).astype(int)\n\nfor i,x in enumerate(y_df):\n    for val in x:\n        #print(x,val,i)    \n        y_df_final[i,val]=1\nprint(y_df_final[1])   \nprint(y_df_final.shape)\n\ntrain_path=np.array(TRAIN_P+train_df['Id'])\nprint(train_path.shape)\nprint(train_path[0])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cdef4f79c1c98b1229f876a5fb0b106d880aa4b6"
      },
      "cell_type": "code",
      "source": "#now we prepare train test split\n\nfrom sklearn.model_selection import train_test_split\nxtrain,xtest,y_train,y_test = train_test_split(\n   train_path,y_df_final, test_size=0.06, random_state=42)\n\nxtrain=xtrain.reshape(xtrain.shape[0],1)\nxtest=xtest.reshape(xtest.shape[0],1)\nprint(xtrain.shape)\nprint(y_train.shape)\nprint(xtest.shape)\nprint(y_test.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e96da2a92af3b2ee286392850ec9c27ddc98f7ce"
      },
      "cell_type": "code",
      "source": "#we have full path from trian folder and test folder to working with autoencodes\nimport matplotlib.pyplot as plt\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img#,save_img\nfrom skimage.transform import resize\nfrom skimage import util \nimport cv2\nfrom PIL import Image\n\ndef load_image(path):\n    R = np.array(Image.open(path+'_red.png'))\n    G = np.array(Image.open(path+'_green.png'))\n    B = np.array(Image.open(path+'_blue.png'))\n    Y = np.array(Image.open(path+'_yellow.png'))\n   \n    image = np.stack((\n        R/2 + Y/2, \n        G/2 + Y/2, \n        B),-1)\n    \n    \n    \n    #image = np.divide(image, 255)\n    return image  \nimg=load_image(np.squeeze(xtrain[1]))\nprint(img.shape)\nplt.imshow(img)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9d4ad11388b50f077cb1aaed86ff4f945089eda5"
      },
      "cell_type": "code",
      "source": "from math import sqrt, ceil, floor\ndef validate_image(image, number_tiles):\n    \"\"\"Basic sanity checks prior to performing a split.\"\"\"\n    TILE_LIMIT = 99 * 99\n\n    try:\n        number_tiles = int(number_tiles)\n    except:\n        raise ValueError('number_tiles could not be cast to integer.')\n\n    if number_tiles > TILE_LIMIT or number_tiles < 2:\n        raise ValueError('Number of tiles must be between 2 and {} (you \\\n                          asked for {}).'.format(TILE_LIMIT, number_tiles))\n        \ndef calc_columns_rows(n):\n    \"\"\"\n    Calculate the number of columns and rows required to divide an image\n    into ``n`` parts.\n    Return a tuple of integers in the format (num_columns, num_rows)\n    \"\"\"\n    num_columns = int(ceil(sqrt(n)))\n    num_rows = int(ceil(n / float(num_columns)))\n    return (num_columns, num_rows)\ndef xslice(imag, number_tiles):\n    \"\"\"\n    Split an image into a specified number of tiles.\n    Args:\n       filename (str):  The filename of the image to split.\n       number_tiles (int):  The number of tiles required.\n    Kwargs:\n       save (bool): Whether or not to save tiles to disk.\n    Returns:\n        Tuple of :class:`Tile` instances.\n    \"\"\"\n    im = Image.fromarray(np.uint8(imag))\n    validate_image(im, number_tiles)\n\n    im_w, im_h = im.size\n    columns, rows = calc_columns_rows(number_tiles)\n    extras = (columns * rows) - number_tiles\n    tile_w, tile_h = int(floor(im_w / columns)), int(floor(im_h / rows))\n\n    tiles = []\n    number = 1\n    for pos_y in range(0, im_h - rows, tile_h): # -rows for rounding error.\n        for pos_x in range(0, im_w - columns, tile_w): # as above.\n            area = (pos_x, pos_y, pos_x + tile_w, pos_y + tile_h)\n            image = im.crop(area)\n            position = (int(floor(pos_x / tile_w)) + 1,\n                        int(floor(pos_y / tile_h)) + 1)\n            coords = (pos_x, pos_y)\n            #tile = Tile(image, number, position, coords)\n            tiles.append(np.array(image)/255)\n            number += 1\n    \n    return tuple(tiles)\nslices=xslice(img,NUM_SLIDES)\n\nslices",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6a5dd44ef965d04a5d52a4346a937785cdd9fc63"
      },
      "cell_type": "code",
      "source": "plt.imshow(slices[0])\nplt.show()\nplt.imshow(slices[1])\nplt.show()\nplt.imshow(slices[2])\nplt.show()\nplt.imshow(slices[3])\nplt.show()\n\nprint(np.max(slices[3]))\nprint(slices[3].shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "36288aa491f21e4afb161c1fa95e37114640c278"
      },
      "cell_type": "code",
      "source": "slide_x=slices[0].shape[0]\nslide_y=slices[0].shape[1]\nslide_z=slices[0].shape[2]\ndef data_generator(paths,labels,batch_size):\n    while True:        \n        random_indexes = np.random.choice(len(paths), batch_size)        \n        batch_images = np.empty((batch_size,NUM_SLIDES, slide_x, slide_y, slide_z))                        \n        batch_labels = np.zeros((batch_size, 28))                        \n        for i, idx in enumerate(random_indexes):\n            #load image in rgbd mode \n        \n            name=np.squeeze(paths[idx])\n            img=load_image(str(name))\n            slides=xslice(img,NUM_SLIDES)\n            for x,slide in enumerate(slides):\n                batch_images[i][x]=slide\n                \n            batch_labels[i]=labels[idx]\n        yield batch_images,batch_labels\n        \ntt,tt2=next(data_generator(xtest,y_test,64))\ngentrain=data_generator(xtrain,y_train,32)\ngentest=data_generator(xtest,y_test,64)\nprint(tt.shape)\nprint(tt2.shape)\nprint(\"DONE\")        ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "695eaebc0b58d2f64667833586cdbfeefdbf4bce"
      },
      "cell_type": "code",
      "source": "import keras\nfrom keras.layers import Dense, Dropout, LSTM,GRU\nfrom keras.layers import Conv2D, Flatten,MaxPooling2D,GlobalMaxPooling2D\nfrom keras.models import Sequential\nfrom keras.layers.wrappers import TimeDistributed\nfrom keras.models import Model\nimport numpy as np\nimport keras\nfrom keras import backend as K\nimport tensorflow as tf\nfrom keras.models import Model\nfrom keras.layers import merge,Input,Dense, concatenate, Conv2D, MaxPooling2D, UpSampling2D, Reshape, core, Dropout,Convolution2D,Activation,Flatten\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler\nfrom keras import backend as K\nfrom keras.utils.vis_utils import plot_model as plot\nfrom keras.optimizers import SGD\n\ndef f1_loss(y_true, y_pred):\n    \n    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n\n    p = tp / (tp + fp + K.epsilon())\n    r = tp / (tp + fn + K.epsilon())\n\n    f1 = 2*p*r / (p+r+K.epsilon())\n    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n    return 1 - K.mean(f1)\n\ndef calculating_class_weights(y_true):\n    from sklearn.utils.class_weight import compute_class_weight\n    number_dim = np.shape(y_true)[1]\n    weights = np.empty([number_dim, 2])\n    for i in range(number_dim):\n        weights[i] = compute_class_weight('balanced', [0.,1.], y_true[:, i])\n    return weights\n\ndef get_weighted_loss(weights):\n    def weighted_loss(y_true, y_pred):\n        return K.mean((weights[:,0]**(1-y_true))*(weights[:,1]**(y_true))*K.binary_crossentropy(y_true, y_pred), axis=-1)\n    return weighted_loss\n\ndef get_weighted_loss_f1(weights):\n    def weighted_loss_f1(y_true, y_pred):\n        return K.mean((weights[:,0]**(1-y_true))*(weights[:,1]**(y_true))*f1_loss(y_true, y_pred), axis=-1)\n    return weighted_loss_f1\n\ndef f1(y_true, y_pred):\n    def recall(y_true, y_pred):\n        \"\"\"Recall metric.\n\n        Only computes a batch-wise average of recall.\n\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives / (possible_positives + K.epsilon())\n        return recall\n\n    def precision(y_true, y_pred):\n        \"\"\"Precision metric.\n\n        Only computes a batch-wise average of precision.\n\n        Computes the precision, a metric for multi-label classification of\n        how many selected items are relevant.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives / (predicted_positives + K.epsilon())\n        return precision\n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n\n\nweigh=calculating_class_weights(y_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "806ef277384d7c6b30d245ec282bd49c0575f911"
      },
      "cell_type": "code",
      "source": "# some basic useless model\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential, load_model, Model\nfrom keras.layers import Bidirectional,TimeDistributed,GRU,LSTM,Activation, Dropout, Flatten, Dense, Input, Conv2D, MaxPooling2D, BatchNormalization, Concatenate, ReLU, LeakyReLU, GlobalAveragePooling2D\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\nfrom keras import metrics\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint\nfrom keras import backend as K\nimport keras\nimport tensorflow as tf\n\nfrom tensorflow import set_random_seed\n\ndef create_model(input_shape):\n    \n    dropRate = 0.25\n    \n    init = Input(input_shape)\n    x = TimeDistributed(BatchNormalization(axis=-1))(init)\n    x = TimeDistributed(Conv2D(32, (3, 3)))(x) #, strides=(2,2))(x)\n    x = TimeDistributed(ReLU())(x)\n\n    x = TimeDistributed(BatchNormalization(axis=-1))(x)\n    x = TimeDistributed(MaxPooling2D(pool_size=(2, 2)))(x)\n    ginp1 = TimeDistributed(Dropout(dropRate))(x)\n    \n    x = TimeDistributed(BatchNormalization(axis=-1))(ginp1)\n    x = TimeDistributed(Conv2D(64, (3, 3), strides=(2,2)))(x)\n    x = TimeDistributed(ReLU())(x)\n    x = TimeDistributed(BatchNormalization(axis=-1))(x)\n    x = TimeDistributed(Conv2D(64, (3, 3)))(x)\n    x = TimeDistributed(ReLU())(x)\n    x = TimeDistributed(BatchNormalization(axis=-1))(x)\n    x = TimeDistributed(Conv2D(64, (3, 3)))(x)\n    x = TimeDistributed(ReLU())(x)\n    \n    x = TimeDistributed(BatchNormalization(axis=-1))(x)\n    x = TimeDistributed(MaxPooling2D(pool_size=(2, 2)))(x)\n    ginp2 =TimeDistributed( Dropout(dropRate))(x)\n    \n    x = TimeDistributed(BatchNormalization(axis=-1))(ginp2)\n    x = TimeDistributed(Conv2D(128, (3, 3)))(x)\n    x = TimeDistributed(ReLU())(x)\n    x =TimeDistributed( BatchNormalization(axis=-1))(x)\n    x = TimeDistributed(Conv2D(128, (3, 3)))(x)\n    x = TimeDistributed(ReLU())(x)\n    x = TimeDistributed(BatchNormalization(axis=-1))(x)\n    x = TimeDistributed(Conv2D(128, (3, 3)))(x)\n    x = TimeDistributed(ReLU())(x)\n    ginp3 = TimeDistributed(Dropout(dropRate))(x)\n    \n    gap1 = TimeDistributed(GlobalAveragePooling2D())(ginp1)    \n    gap2 = TimeDistributed(GlobalAveragePooling2D())(ginp2)\n    gap3 = TimeDistributed(GlobalAveragePooling2D())(ginp3)\n    \n    x = Concatenate()([gap1, gap2, gap3])\n    x = BatchNormalization(axis=-1)(x)\n    x = Bidirectional(LSTM(128,return_sequences=True))(x)\n  #  x = Bidirectional()(x)\n    x = Dropout(dropRate)(x)\n    x = BatchNormalization(axis=-1)(x)\n    x = Bidirectional(LSTM(128))(x)\n   # x = Bidirectional()(x)\n    x = Dropout(dropRate)(x)\n    \n    #x = BatchNormalization(axis=-1)(x)\n    #x = Dense(1024, activation='relu')(x)\n    #x = Dropout(dropRate)(x)\n    \n    #x = BatchNormalization(axis=-1)(x)\n    #x = Dense(1024, activation='relu')(x)\n    #x = Dropout(0.1)(x)\n    \n    x = Dense(28)(x)\n    x = Activation('sigmoid')(x)\n    \n    model = Model(init, x)\n    \n    return model\n\nmodel=create_model((NUM_SLIDES, slide_x, slide_y, slide_z))\nmodel.summary()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "873f98275cbd0d2e6c75096f2f3e40d1f0af62c2"
      },
      "cell_type": "code",
      "source": "def get_model2(input_shape):\n    init = Input(input_shape)    \n    x=TimeDistributed(Conv2D(32, (3, 3)))(init)\n    x=TimeDistributed((Activation('relu')))(x)\n    x=TimeDistributed((MaxPooling2D(pool_size=(2, 2))))(x)\n\n    x=TimeDistributed((Conv2D(32, (3, 3))))(x)\n    x=TimeDistributed((Activation('relu')))(x)\n    x=TimeDistributed((MaxPooling2D(pool_size=(2, 2))))(x)\n\n    x=TimeDistributed((Conv2D(64, (3, 3))))(x)\n    x=TimeDistributed((Activation('relu')))(x)\n    x=TimeDistributed((MaxPooling2D(pool_size=(2, 2))))(x)\n\n# the model so far outputs 3D feature maps (height, width, features)\n\n\n    x=TimeDistributed((Flatten()))(x)  # this converts our 3D feature maps to 1D feature vectors\n    x = Bidirectional(LSTM(128,return_sequences=True))(x)    \n    x = Bidirectional(LSTM(128))(x)\n    x =Dense(512)(x)\n    x =Activation('relu')(x)\n    x =Dense(28)(x)\n    x=Activation('sigmoid')(x)\n    model = Model(init, x)\n    return model\n#model=get_model2((NUM_SLIDES, slide_x, slide_y, slide_z))\n#model.summary()\n#model.compile(optimizer=\"rmsprop\", loss=f1_loss,metrics=[\"acc\",f1_loss,\"binary_crossentropy\",f1])\n#model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "10be5daae6e2cbcf67cb65fd558ae909c85d03de"
      },
      "cell_type": "code",
      "source": "from keras import optimizers\n#model = load_model('/kaggle/working/class.model',custom_objects={'f1_loss': f1_loss,'f1':f1})\n#sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n#opt=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=True)\nmodel.compile(optimizer=\"adam\", loss='binary_crossentropy',metrics=[\"acc\",f1_loss,\"binary_crossentropy\",f1])\nprint(\"DONE\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "_uuid": "498ef59376dcf379b4ba6ba8c9f3702ca3d139cd"
      },
      "cell_type": "code",
      "source": "from keras.callbacks import ModelCheckpoint\ncheckpointer = ModelCheckpoint(\n    '/kaggle/working/class.model',\n    verbose=2, save_best_only=True)\n\nhistory=model.fit_generator(gentrain,\n                    steps_per_epoch=100,epochs=100,\n                    verbose=1,\n                    validation_data=next(gentest),\n                    validation_steps=10,\n                    callbacks=[checkpointer]\n                   )",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a4ab2c60320b1aac7ec9abe03360439100ccac6b"
      },
      "cell_type": "code",
      "source": "from keras.models import load_model\nmodel = load_model('/kaggle/working/class.model',custom_objects={'f1_loss': f1_loss,'f1':f1})\nmodel.summary()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d5ede410a43909c2b96eed9a09c1e62ec29d3c72"
      },
      "cell_type": "code",
      "source": "from sklearn.metrics import f1_score\n\ntr,y_true=next(data_generator(xtest,y_test,450))\ny_pred=model.predict(tr)\ny_pred.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ad068da6c1e4dcffca75e5706a5c3443b20f1f4c"
      },
      "cell_type": "code",
      "source": "for x,row in enumerate(y_pred):\n    for y,rcol in enumerate(row):\n        if y_pred[x][y]>0.5:\n            y_pred[x][y]=1\n        else:\n            y_pred[x][y]=0\n        \nprint(\"DONE\")    \n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "098b140d391a649856f3e6405b9722ed656b8b71"
      },
      "cell_type": "code",
      "source": "\nprint(f1_score(y_true, y_pred, average='macro')  )\n\nprint(f1_score(y_true, y_pred, average='micro')  )\n\nprint(f1_score(y_true, y_pred, average='weighted')  )\n\nprint(f1_score(y_true, y_pred, average=None))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "694dbfb17c0b441d967bbfac7ba4f192a9523684"
      },
      "cell_type": "code",
      "source": "def show_history(history):\n    fig, ax = plt.subplots(1, 3, figsize=(15,5))\n    ax[0].set_title('loss')\n    ax[0].plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\n    ax[0].plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\n    ax[1].set_title('f1_loss')\n    ax[1].plot(history.epoch, history.history[\"f1_loss\"], label=\"Train f1 loss\")\n    ax[1].plot(history.epoch, history.history[\"val_f1_loss\"], label=\"Validation f1 loss\")\n    ax[2].set_title('f1')\n    ax[2].plot(history.epoch, history.history[\"f1\"], label=\"Train f1\")\n    ax[2].plot(history.epoch, history.history[\"val_f1\"], label=\"Validation f1\")\n    \n    ax[0].legend()\n    ax[1].legend()\n    ax[2].legend()\n    \n\nshow_history(history)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}